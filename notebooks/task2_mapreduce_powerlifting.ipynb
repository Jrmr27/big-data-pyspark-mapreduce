{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQMVsYkyVYi8"
      },
      "outputs": [],
      "source": [
        "# as always, starting spark session for map reduce tasks\n",
        "# spark for this large dataset of 380000 & 8000 truncated row number's\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Powerlifting MapReduce Task\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uploading the dataset from my computer to colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "uploaded = files.upload()\n",
        "\n",
        "uploaded  # just to see the files names uploaded here\n",
        "\n",
        "print(\"files uploaded:\", list(uploaded.keys()))\n"
      ],
      "metadata": {
        "id": "kHLQLNb9XSLa",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the csv files\n",
        "# i use pandas (filter, group, simulations) because mapreduce here is easier to simulate in python (mapper + reducer)\n",
        "meets_path = \"meets.csv\"\n",
        "opl_path = \"openpowerlifting.csv\"\n",
        "\n",
        "meets_df = pd.read_csv(meets_path)\n",
        "opl_df = pd.read_csv(opl_path)\n",
        "\n",
        "print(\"meets rows:\", len(meets_df))\n",
        "print(\"openpowerlifting rows:\", len(opl_df))\n",
        "\n",
        "# checking columns to avoid silly mistakes with names\n",
        "print(\"meets columns:\", list(meets_df.columns))\n",
        "print(\"opl columns:\", list(opl_df.columns))\n",
        "\n",
        "meets_df.head(3)\n",
        "meets_df.head(3)\n"
      ],
      "metadata": {
        "id": "jxydMNg0ZgOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LITTLE CLEANNING ~~~~~~~~~~"
      ],
      "metadata": {
        "id": "xHnsRyUeaUsm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# basic cleaning only for columns we need for the 2 task objective yey\n",
        "# i only keep the needed cols to avoid big memory usage (poor computer)\n",
        "\n",
        "meets_small = meets_df[[\"MeetID\", \"MeetName\", \"MeetCountry\", \"Date\"]].copy()\n",
        "opl_small = opl_df[[\"MeetID\", \"Name\", \"TotalKg\"]].copy()\n",
        "\n",
        "# removing rows with missing meetid or name because they break the groupings\n",
        "meets_small = meets_small.dropna(subset=[\"MeetID\", \"MeetName\", \"MeetCountry\", \"Date\"])\n",
        "opl_small = opl_small.dropna(subset=[\"MeetID\", \"Name\"])\n",
        "\n",
        "# converting meetid to int (sometimes it reads as float)\n",
        "meets_small[\"MeetID\"] = meets_small[\"MeetID\"].astype(int)\n",
        "opl_small[\"MeetID\"] = opl_small[\"MeetID\"].astype(int)\n",
        "\n",
        "# totalkg can be missing or text, so i convert and keep numeric\n",
        "opl_small[\"TotalKg\"] = pd.to_numeric(opl_small[\"TotalKg\"], errors=\"coerce\")\n",
        "\n",
        "print(\"after cleaning -> meets:\", len(meets_small), \"| opl:\", len(opl_small))\n",
        "opl_small.head(3)\n"
      ],
      "metadata": {
        "id": "Xbp_Mfk7agG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 COUNT PARTICIPATES x Meet ID ~~~~~~~~"
      ],
      "metadata": {
        "id": "Dv3q90KLasiP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# mapreduce idea done would be\n",
        "# mapper: (MeetID kew competition value-> 1 per assigned participant)\n",
        "# for every row (one row = one lifter in that meet)\n",
        "# reducer: sum all 1s per MeetID => total Nº PARTICIPANTS x ID\n",
        "\n",
        "def mapper_participants(opl_rows):\n",
        "    mapped = [] #empty list\n",
        "    for meet_id in opl_rows[\"MeetID\"]:\n",
        "        mapped.append((meet_id, 1)) # pair generated\n",
        "    return mapped\n",
        "\n",
        "def reducer_sum_counts(mapped_pairs):\n",
        "    reduced = {} # dictionary\n",
        "    for k, v in mapped_pairs: #k meet id, v is 1\n",
        "        reduced[k] = reduced.get(k, 0) + v #meet id exist => actual value, if not, returns 0.+1\n",
        "    return reduced\n",
        "\n",
        "mapped_participants = mapper_participants(opl_small)\n",
        "participants_count = reducer_sum_counts(mapped_participants)\n",
        "\n",
        "print(\"example of reduced result (first 5 meetids):\")\n",
        "print(list(participants_count.items())[:5])\n"
      ],
      "metadata": {
        "id": "g4X-3_tca08o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turning the reducer output into a dataframe for printing nicely\n",
        "# list => TABLE\n",
        "participants_df = pd.DataFrame(list(participants_count.items()), columns=[\"MeetID\", \"Participants\"])\n",
        "# order from more to less nº participant competitions (more important ones)\n",
        "participants_df = participants_df.sort_values(\"Participants\", ascending=False)\n",
        "\n",
        "participants_df.head(10)\n"
      ],
      "metadata": {
        "id": "T4EUaJoNa8p6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 COMPETITIONS BEING LISTED ~~~~~~"
      ],
      "metadata": {
        "id": "l_F_4Cc1a_CV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here year is needed for analysis (not all date)\n",
        "# meets has Date like \"2016-10-29\" so i take the first 4 chars\n",
        "# to string date, pick 4 chars (year)\n",
        "meets_small[\"Year\"] = meets_small[\"Date\"].astype(str).str[:4]\n",
        "\n",
        "# sometimes date can be weird, so i keep only valid numeric year\n",
        "meets_small = meets_small[meets_small[\"Year\"].str.isnumeric()] # true if numbers\n",
        "meets_small[\"Year\"] = meets_small[\"Year\"].astype(int) # to int (sort, group, analysis later)\n",
        "\n",
        "meets_small.head(3)\n"
      ],
      "metadata": {
        "id": "sCC1Y3KpbCU5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# i already have participants per MeetID from prevous task 2 point 1\n",
        "# now i \"join\" with meets info to get name/country/year\n",
        "# 1 row x competition\n",
        "meets_info = meets_small.drop_duplicates(subset=[\"MeetID\"])[[\"MeetID\", \"MeetName\", \"MeetCountry\", \"Year\"]]\n",
        "# unify tables meet id as key that have meets info (nor participants? => NaN)\n",
        "competitions_df = meets_info.merge(participants_df, on=\"MeetID\", how=\"left\")\n",
        "\n",
        "# if some meetid has no participants, it will be NaN -> i set to 0\n",
        "competitions_df[\"Participants\"] = competitions_df[\"Participants\"].fillna(0).astype(int)\n",
        "\n",
        "# sorting to see the biggest events\n",
        "competitions_df = competitions_df.sort_values([\"Participants\"], ascending=False)\n",
        "\n",
        "competitions_df.head(15)\n"
      ],
      "metadata": {
        "id": "gieFm7qdbKAE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the final competitions table because it is a requested output basically\n",
        "competitions_df.to_csv(\"task2_competitions_list.csv\", index=False)\n",
        "\n",
        "print(\"saved -> task2_competitions_list.csv\")\n"
      ],
      "metadata": {
        "id": "xMiUHtU8bR99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 TOTAL WEIGHT LIFTED x PARTICIPANT, ALL COMPETITIONS taken into account ~~~~~~"
      ],
      "metadata": {
        "id": "UvF6iZvBbWG6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# here the mpreducer\n",
        "# mapper pairs: (Name per person , TotalKg) for each row (one result of a lifter)\n",
        "# reducer dicc: sum all TotalKg per Name across all meets\n",
        "#\n",
        "def mapper_total_by_lifter(opl_rows):\n",
        "    mapped = []\n",
        "    for _, row in opl_rows.iterrows():\n",
        "        name = row[\"Name\"]\n",
        "        total = row[\"TotalKg\"]\n",
        "\n",
        "        # if total is missing i skip it (otherwise it becomes nan sums)\n",
        "        if pd.notna(total):\n",
        "            mapped.append((name, float(total))) # name? is float to good sum?\n",
        "    return mapped\n",
        "\n",
        "def reducer_sum_totals(mapped_pairs):\n",
        "    reduced = {}\n",
        "    for k, v in mapped_pairs: #name, total kg\n",
        "        reduced[k] = reduced.get(k, 0.0) + v # name not exixst, starts at 0.0 + total kg\n",
        "    return reduced\n",
        "\n",
        "mapped_totals = mapper_total_by_lifter(opl_small)\n",
        "total_by_lifter = reducer_sum_totals(mapped_totals)\n",
        "\n",
        "print(\"example (first 5 lifters):\")\n",
        "print(list(total_by_lifter.items())[:5])\n"
      ],
      "metadata": {
        "id": "3IDV1XnNbgLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# converting reducer output to dataframe\n",
        "# items => pairs of dicc total_by_lifter, pd.df to create table\n",
        "lifters_df = pd.DataFrame(list(total_by_lifter.items()), columns=[\"Name\", \"TotalKg_AllCompetitions\"])\n",
        "lifters_df = lifters_df.sort_values(\"TotalKg_AllCompetitions\", ascending=False)\n",
        "# people compiting lots of times or years\n",
        "lifters_df.head(15) #visual\n"
      ],
      "metadata": {
        "id": "a5gdZJh6bulP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# saving the output of course (as every task in 2)\n",
        "lifters_df.to_csv(\"task2_total_by_lifter.csv\", index=False)\n",
        "\n",
        "print(\"saved -> task2_total_by_lifter.csv\")\n"
      ],
      "metadata": {
        "id": "vpHydPVNb0Sa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}