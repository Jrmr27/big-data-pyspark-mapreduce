{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOECuxijHDeu"
      },
      "outputs": [],
      "source": [
        "# installing pyspark in colab (so i can use spark)\n",
        "!pip -q install pyspark\n",
        "\n",
        "# starting a spark session (this is like the entry point for pyspark)\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"vehicle_co2_task1\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uploading the dataset from my computer to colab\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "uploaded  # just to see the file name i uploaded\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JwFKxYSTZxCO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reading the csv with spark\n",
        "# header=True means first row has column names\n",
        "# inferSchema=True detect automatically spark data TYPES (NOT ALL STRINGS treated)\n",
        "# sep=\",\" because it is a normal csv with commas\n",
        "file_name = \"co2.csv\"\n",
        "\n",
        "df_raw = spark.read.csv(\n",
        "    file_name,\n",
        "    header=True,\n",
        "    inferSchema=True,\n",
        "    sep=\",\"\n",
        ")\n",
        "\n",
        "print(\"rows in raw df:\", df_raw.count())\n",
        "print(\"columns:\", len(df_raw.columns))\n",
        "df_raw.printSchema() # null values in rows allowed? => data cleanning\n",
        "df_raw.show(5, truncate=False) # VEHICLE CLASS: engine ~ fuel ~ CO2 emission\n"
      ],
      "metadata": {
        "id": "A8lOdED2aAau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# renaming columns to simpler names (lowercase, snake_case, no u or simbs)\n",
        "# this makes the code easier for me later\n",
        "\n",
        "new_cols = {\n",
        "    \"Make\": \"make\",\n",
        "    \"Model\": \"model\",\n",
        "    \"Vehicle Class\": \"vehicle_class\",\n",
        "    \"Engine Size(L)\": \"engine_size_l\",\n",
        "    \"Cylinders\": \"cylinders\",\n",
        "    \"Transmission\": \"transmission\",\n",
        "    \"Fuel Type\": \"fuel_type\",\n",
        "    \"Fuel Consumption City (L/100 km)\": \"fuel_city_l100\",\n",
        "    \"Fuel Consumption Hwy (L/100 km)\": \"fuel_hwy_l100\",\n",
        "    \"Fuel Consumption Comb (L/100 km)\": \"fuel_comb_l100\",\n",
        "    \"Fuel Consumption Comb (mpg)\": \"fuel_comb_mpg\",\n",
        "    \"CO2 Emissions(g/km)\": \"co2_g_km\"\n",
        "}\n",
        "\n",
        "df = df_raw\n",
        "for old, new in new_cols.items():\n",
        "    if old in df.columns:\n",
        "        df = df.withColumnRenamed(old, new)\n",
        "\n",
        "df.printSchema()\n",
        "df.show(5, truncate=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "6EjgBCQBaHkd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking MISSING VALUES per column\n",
        "# i do a simple count of nulls because missing data can affect analysis and recommendations\n",
        "# import\n",
        "\n",
        "from pyspark.sql.functions import col, sum as spark_sum, when\n",
        "\n",
        "null_counts = df.select([\n",
        "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) # names\n",
        "    for c in df.columns\n",
        "])\n",
        "\n",
        "null_counts.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "-EKcGJiraUzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cleaning the dataset\n",
        "# i keep rows that have the main fields i need: make, model, vehicle class, fuel type and co2\n",
        "# because missing these would make the recommender unreliable\n",
        "# trim cleannign pre & post STRING SPACES => figure strings as same not because spaces diffs\n",
        "\n",
        "from pyspark.sql.functions import trim\n",
        "\n",
        "df_clean = df \\\n",
        "    .withColumn(\"make\", trim(col(\"make\"))) \\\n",
        "    .withColumn(\"model\", trim(col(\"model\"))) \\\n",
        "    .withColumn(\"vehicle_class\", trim(col(\"vehicle_class\"))) \\\n",
        "    .withColumn(\"fuel_type\", trim(col(\"fuel_type\"))) \\\n",
        "    .filter(col(\"make\").isNotNull()) \\\n",
        "    .filter(col(\"model\").isNotNull()) \\\n",
        "    .filter(col(\"vehicle_class\").isNotNull()) \\\n",
        "    .filter(col(\"fuel_type\").isNotNull()) \\\n",
        "    .filter(col(\"co2_g_km\").isNotNull())\n",
        "\n",
        "print(\"rows before:\", df.count())\n",
        "print(\"rows after cleaning:\", df_clean.count())\n",
        "df_clean.show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "D3C1D4FvacVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing DUPLICATES => content (x2 could cause LESS SIMILARITY)\n",
        "# some cars may appear more than once, so i drop duplicates based on key columns => line x car\n",
        "\n",
        "key_cols = [c for c in [\"make\", \"model\", \"vehicle_class\", \"engine_size_l\", \"transmission\", \"fuel_type\"] if c in df_clean.columns]\n",
        "df_clean = df_clean.dropDuplicates(key_cols)\n",
        "\n",
        "print(\"rows after dropDuplicates:\", df_clean.count())\n"
      ],
      "metadata": {
        "id": "usagl5E2agsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EXPLORATORY ANALYSIS ~~~~~~~~~~~~~~"
      ],
      "metadata": {
        "id": "whINXSYEamJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SUMMARY OF STATISTICS\n",
        "# this helps me understand the distribution and if there are extreme values\n",
        "# realistic values, max SUVS & compact mins, syddev high variability\n",
        "\n",
        "df_clean.select(\"co2_g_km\").describe().show()\n"
      ],
      "metadata": {
        "id": "E-9LFmIvaxCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cars LOWEST co2 emissions => HYBRID (fuel_type X), range emissions, smaller engine\n",
        "df_clean.select(\"make\", \"model\", \"vehicle_class\", \"fuel_type\", \"engine_size_l\", \"co2_g_km\") \\\n",
        "    .orderBy(col(\"co2_g_km\").asc()).show(10, truncate=False)\n",
        "\n",
        "# cars HIGHEST co2 emissions -> 2 SEATER, SUV & VANS, bigger engines and range emissions\n",
        "df_clean.select(\"make\", \"model\", \"vehicle_class\", \"fuel_type\", \"engine_size_l\", \"co2_g_km\") \\\n",
        "    .orderBy(col(\"co2_g_km\").desc()).show(10, truncate=False)\n",
        "# CO2 RELATION ~ engine size, fuel type & class?"
      ],
      "metadata": {
        "id": "_ceDAbP9a5VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking how co2 emissions change depending on fuel type\n",
        "# i use avg() & count() => typical values & nº cars per group\n",
        "# interp: per line, diff type fuel (focus nº cars, + trad fuel, more avg co2)\n",
        "# hybrid x (-co2) & petrol z relevant (+ cars, +co2), ethanol e ok minus (co2 medium), N only 1 (no representative)\n",
        "\n",
        "from pyspark.sql.functions import avg, count\n",
        "\n",
        "df_clean.groupBy(\"fuel_type\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"n_cars\"),\n",
        "        avg(\"co2_g_km\").alias(\"avg_co2\")\n",
        "    ) \\\n",
        "    .orderBy(col(\"avg_co2\").asc()).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "pXOtLjrHa_dJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# checking co2 emissions by vehicle class: like suv, compact, etc...\n",
        "# this can show trends like bigger cars = higher co2\n",
        "# - OC2 avg (station wagon, compact & mid-size), - (suv, van, pick up)\n",
        "\n",
        "df_clean.groupBy(\"vehicle_class\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"n_cars\"),\n",
        "        avg(\"co2_g_km\").alias(\"avg_co2\")\n",
        "    ) \\\n",
        "    .orderBy(col(\"avg_co2\").asc()) \\\n",
        "    .show(20, truncate=False)\n"
      ],
      "metadata": {
        "id": "8hKNr17gbIgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# looking at engine size vs co2 in a simple way\n",
        "# i create basic bins(ranges) to see if bigger engines have higher co2 on average per bin\n",
        "# + size engine, + co2\n",
        "\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_bins = df_clean.withColumn( #create col\n",
        "    \"engine_bin\", #name\n",
        "    when(col(\"engine_size_l\") < 2.0, \"<2.0\") \\\n",
        "    .when((col(\"engine_size_l\") >= 2.0) & (col(\"engine_size_l\") < 3.0), \"2.0-2.9\") \\\n",
        "    .when((col(\"engine_size_l\") >= 3.0) & (col(\"engine_size_l\") < 4.0), \"3.0-3.9\") \\\n",
        "    .otherwise(\">=4.0\")\n",
        ")\n",
        "\n",
        "df_bins.groupBy(\"engine_bin\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"n_cars\"),\n",
        "        avg(\"co2_g_km\").alias(\"avg_co2\")\n",
        "    ) \\\n",
        "    .orderBy(\"engine_bin\") \\\n",
        "    .show(truncate=False)\n"
      ],
      "metadata": {
        "id": "B-MPfjNxbNNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RECOMENDATION SYSTEM ~~~~~~~~~~~~~~"
      ],
      "metadata": {
        "id": "eR2WhW9rbemU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a simple ID per car & readable name\n",
        "# this helps me show recommendations more clearly\n",
        "# unify strings import & id unique increasing spark\n",
        "from pyspark.sql.functions import concat_ws, monotonically_increasing_id\n",
        "\n",
        "df_rec = df_clean.withColumn(\n",
        "    \"car_name\",\n",
        "    concat_ws(\" \", col(\"make\"), col(\"model\"))\n",
        ").withColumn(\n",
        "    \"car_id\",\n",
        "    monotonically_increasing_id()\n",
        ")\n",
        "\n",
        "df_rec.select(\"car_id\", \"car_name\", \"vehicle_class\", \"fuel_type\", \"engine_size_l\", \"co2_g_km\") \\\n",
        "    .show(5, truncate=False)\n"
      ],
      "metadata": {
        "id": "e_OcfcnEblDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# building FEATURES 4 SIMILARITY\n",
        "# i use onehot for categorical columns & scale numeric columns\n",
        "# then i create one final vector called \"features\"\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "cat_cols = [\"vehicle_class\", \"fuel_type\", \"transmission\"]\n",
        "cat_cols = [c for c in cat_cols if c in df_rec.columns]\n",
        "\n",
        "num_cols = [\"engine_size_l\", \"cylinders\", \"fuel_city_l100\", \"fuel_hwy_l100\", \"fuel_comb_l100\", \"fuel_comb_mpg\", \"co2_g_km\"]\n",
        "num_cols = [c for c in num_cols if c in df_rec.columns]\n",
        "\n",
        "indexers = [StringIndexer(inputCol=c, outputCol=c+\"_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
        "encoders = [OneHotEncoder(inputCol=c+\"_idx\", outputCol=c+\"_ohe\") for c in cat_cols]\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[c+\"_ohe\" for c in cat_cols] + num_cols,\n",
        "    outputCol=\"raw_features\"\n",
        ")\n",
        "\n",
        "scaler = StandardScaler(inputCol=\"raw_features\", outputCol=\"features\", withMean=True, withStd=True)\n",
        "\n",
        "pipe = Pipeline(stages=indexers + encoders + [assembler, scaler])\n",
        "\n",
        "model_feat = pipe.fit(df_rec)\n",
        "df_feat = model_feat.transform(df_rec)\n",
        "\n",
        "df_feat.select(\"car_id\", \"car_name\", \"features\").show(3, truncate=False)\n"
      ],
      "metadata": {
        "id": "40NAoZa5brj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using locality seneitive hashing to find SIMILARITY CARS based on the FEATURE VECTPR\n",
        "# this is an approximate nearest neighbor method, so it is fast on bigger datasets\n",
        "# import: lsh algorytm knn lsh with numeric vector wok & EUCLIDEAN DISTANCE\n",
        "\n",
        "from pyspark.ml.feature import BucketedRandomProjectionLSH\n",
        "\n",
        "lsh = BucketedRandomProjectionLSH(\n",
        "    inputCol=\"features\",\n",
        "    outputCol=\"hashes\", # internal hash cols, group similar vectors\n",
        "    bucketLength=2.0, # stricti similarity: medium value, + value + prec - neighbours\n",
        "    numHashTables=3 # times repeating hashing (+ quality): medium, + tables + quality less computing power\n",
        ")\n",
        "\n",
        "lsh_model = lsh.fit(df_feat)\n"
      ],
      "metadata": {
        "id": "TLJXZ0SJb1Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selecting 1 CAR to RECOMEND SIMILAR CARS to this one\n",
        "# i choose by searching a word in the car_name (example: \"CIVIC\" or \"COROLLA\")\n",
        "# interp: reference car recommended system will look similac vehicles with similar chars & co2 emissions\n",
        "\n",
        "target_word = \"CIVIC\"   # the model I choose to test yetsss (EASY TO CHANGE)\n",
        "\n",
        "target_df = df_feat.filter(col(\"car_name\").contains(target_word)).limit(1) # 1 car, one only reference vector needed\n",
        "\n",
        "target_df.select(\"car_id\", \"car_name\", \"vehicle_class\", \"fuel_type\", \"co2_g_km\").show(truncate=False) #car with fields\n"
      ],
      "metadata": {
        "id": "XOfCgDQFb-Qt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# finding NEAREST NEIGHBOURS to the selected car\n",
        "# the result includes a DISTANCE COL (minus distance = more similar)\n",
        "# recs expl:  df cars & vectors, taget car reference, 10 similar cars to search\n",
        "\n",
        "if target_df.count() == 0: # found car?\n",
        "    print(\"no car found with that word ops\")\n",
        "else:\n",
        "    recs = lsh_model.approxNearestNeighbors(df_feat, target_df.first()[\"features\"], 10) # target = obj type row; hash requires numerical vector (features col)\n",
        "\n",
        "    recs.select(\n",
        "        \"car_id\", \"car_name\", \"vehicle_class\", \"fuel_type\",\n",
        "        \"engine_size_l\", \"co2_g_km\", \"distCol\"\n",
        "    ).show(truncate=False)\n"
      ],
      "metadata": {
        "id": "nZfvbX-KcJ9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION ~~~~~~~~"
      ],
      "metadata": {
        "id": "FW7sxJTtOH5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluating an idea for example :D (evaluations are logic & system is consistent of lsh?)\n",
        "# if cars are really \"similar\", their co2 values should not be extremely different\n",
        "# so i be doing min/max co2 inside the recommended list\n",
        "# interp: cloase co2 values recommended, small range & low std => SIMILAR EMISSION LEVELS\n",
        "\n",
        "if target_df.count() != 0: # yes car objective\n",
        "    recs_small = recs.select(\"co2_g_km\")\n",
        "    recs_small.describe().show()\n"
      ],
      "metadata": {
        "id": "16nucoT1cjVZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}